{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c17f7e9",
   "metadata": {},
   "source": [
    "# Evaluation and Analysis\n",
    "\n",
    "This notebook evaluates the cancer driver gene prioritization results.\n",
    "\n",
    "## Goals:\n",
    "- Compare with known cancer drivers (COSMIC, IntOGen)\n",
    "- Calculate precision, recall, and F1-score\n",
    "- Visualize results\n",
    "- Generate comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add your evaluation code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aa19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = []\n",
    "\n",
    "for name, eval_df in evaluation_results.items():\n",
    "    # Get metrics at top-50\n",
    "    top_50_metrics = eval_df[eval_df['top_k'] == 50]\n",
    "    if not top_50_metrics.empty:\n",
    "        summary.append({\n",
    "            'method': name,\n",
    "            'precision@50': top_50_metrics['precision'].values[0],\n",
    "            'recall@50': top_50_metrics['recall'].values[0],\n",
    "            'f1@50': top_50_metrics['f1_score'].values[0],\n",
    "            'true_positives@50': top_50_metrics['true_positives'].values[0]\n",
    "        })\n",
    "\n",
    "if summary:\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_df = summary_df.sort_values('f1@50', ascending=False)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION SUMMARY (Top-50 Predictions)\")\n",
    "    print(\"=\"*60)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = results_dir / 'evaluation_summary.csv'\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"\\\\n‚úÖ Saved evaluation summary to: {summary_file.name}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Evaluation completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313cdb0",
   "metadata": {},
   "source": [
    "## Summary and Export\n",
    "\n",
    "Generate comprehensive evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top predictions\n",
    "for name, df in results_dict.items():\n",
    "    gene_col = [c for c in df.columns if 'gene' in c.lower() or 'symbol' in c.lower()]\n",
    "    if not gene_col:\n",
    "        continue\n",
    "    gene_col = gene_col[0]\n",
    "    \n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Top 20 Predictions: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    top_20 = df.head(20).copy()\n",
    "    top_20['is_known_driver'] = top_20[gene_col].isin(known_drivers)\n",
    "    top_20['status'] = top_20['is_known_driver'].map({True: '‚úÖ Known', False: 'üÜï Novel'})\n",
    "    \n",
    "    print(top_20[[gene_col, 'status']].to_string(index=False))\n",
    "    \n",
    "    known_count = top_20['is_known_driver'].sum()\n",
    "    print(f\"\\\\nKnown drivers in top 20: {known_count}/20 ({known_count/20*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a040105",
   "metadata": {},
   "source": [
    "## Detailed Analysis: Top Predicted Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(evaluation_results) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for name, eval_df in evaluation_results.items():\n",
    "        axes[0].plot(eval_df['top_k'], eval_df['precision'], marker='o', label=name)\n",
    "        axes[1].plot(eval_df['top_k'], eval_df['recall'], marker='s', label=name)\n",
    "        axes[2].plot(eval_df['top_k'], eval_df['f1_score'], marker='^', label=name)\n",
    "    \n",
    "    axes[0].set_xlabel('Top K Predictions')\n",
    "    axes[0].set_ylabel('Precision')\n",
    "    axes[0].set_title('Precision at Different K')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Top K Predictions')\n",
    "    axes[1].set_ylabel('Recall')\n",
    "    axes[1].set_title('Recall at Different K')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_xlabel('Top K Predictions')\n",
    "    axes[2].set_ylabel('F1 Score')\n",
    "    axes[2].set_title('F1 Score at Different K')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Saved evaluation plot to: {results_dir / 'evaluation_metrics.png'}\")\n",
    "else:\n",
    "    print(\"No results to visualize yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688ea0c",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd434f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prioritization(predicted_genes, known_drivers, top_k_list=[10, 20, 50, 100, 200]):\n",
    "    \"\"\"\n",
    "    Evaluate gene prioritization against known drivers\n",
    "    \n",
    "    Args:\n",
    "        predicted_genes: List of predicted driver genes (ranked)\n",
    "        known_drivers: Set of known driver genes\n",
    "        top_k_list: List of top-k values to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for top_k in top_k_list:\n",
    "        if top_k > len(predicted_genes):\n",
    "            continue\n",
    "            \n",
    "        top_predicted = set(predicted_genes[:top_k])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        true_positives = len(top_predicted & known_drivers)\n",
    "        false_positives = len(top_predicted - known_drivers)\n",
    "        false_negatives = len(known_drivers - top_predicted)\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / len(known_drivers) if len(known_drivers) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'top_k': top_k,\n",
    "            'true_positives': true_positives,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate each result file\n",
    "evaluation_results = {}\n",
    "\n",
    "for name, df in results_dict.items():\n",
    "    # Get gene column (flexible naming)\n",
    "    gene_col = None\n",
    "    for col in ['gene', 'Gene', 'Hugo_Symbol', 'symbol']:\n",
    "        if col in df.columns:\n",
    "            gene_col = col\n",
    "            break\n",
    "    \n",
    "    if gene_col is None:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {name}: No gene column found\")\n",
    "        continue\n",
    "    \n",
    "    predicted_genes = df[gene_col].tolist()\n",
    "    eval_df = evaluate_prioritization(predicted_genes, known_drivers)\n",
    "    evaluation_results[name] = eval_df\n",
    "    \n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Evaluation: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(eval_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee990c",
   "metadata": {},
   "source": [
    "## Calculate Evaluation Metrics\n",
    "\n",
    "Calculate precision, recall, and F1-score at different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all result files\n",
    "result_files = list(results_dir.glob(\"*.tsv\")) + list(results_dir.glob(\"*.txt\"))\n",
    "result_files = [f for f in result_files if f.parent == results_dir]  # Exclude subdirectories\n",
    "\n",
    "if len(result_files) == 0:\n",
    "    print(f\"‚ö†Ô∏è  No result files found in {results_dir}\")\n",
    "    print(\"\\\\nPlease run analysis first (notebooks 04 or 05)\")\n",
    "    results_dict = {}\n",
    "else:\n",
    "    print(f\"Found {len(result_files)} result file(s):\")\n",
    "    results_dict = {}\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        try:\n",
    "            df = pd.read_csv(result_file, sep=\"\\\\t\")\n",
    "            results_dict[result_file.stem] = df\n",
    "            print(f\"  ‚úÖ {result_file.name}: {len(df)} genes\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {result_file.name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064e1bc",
   "metadata": {},
   "source": [
    "## Load Prioritization Results\n",
    "\n",
    "Load results from Endeavour and/or nCop analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load known drivers\n",
    "known_drivers = set(pd.read_csv(reference_file, header=None)[0].values)\n",
    "print(f\"\\\\nLoaded {len(known_drivers)} known cancer driver genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272eccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a sample known drivers list (replace with actual COSMIC/IntOGen data)\n",
    "known_drivers_example = [\n",
    "    'TP53', 'KRAS', 'EGFR', 'PIK3CA', 'BRAF', 'PTEN', 'APC', 'RB1',\n",
    "    'CDKN2A', 'SMAD4', 'FBXW7', 'NRAS', 'ARID1A', 'CTNNB1', 'FAT1',\n",
    "    'GATA3', 'MAP3K1', 'MYC', 'NOTCH1', 'PPP2R1A', 'ATM', 'BRCA1',\n",
    "    'BRCA2', 'CDH1', 'ERBB2', 'FGFR3', 'IDH1', 'NF1', 'SETD2', 'VHL'\n",
    "]\n",
    "\n",
    "# Save example reference file\n",
    "reference_file = reference_dir / 'known_cancer_drivers.txt'\n",
    "pd.Series(known_drivers_example).to_csv(reference_file, index=False, header=False)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(known_drivers_example)} known driver genes to: {reference_file.name}\")\n",
    "print(f\"\\\\n‚ö†Ô∏è  Note: Replace this with actual COSMIC/IntOGen data for real evaluation!\")\n",
    "print(f\"\\\\nDownload from:\")\n",
    "print(f\"  - COSMIC: https://cancer.sanger.ac.uk/census\")\n",
    "print(f\"  - IntOGen: https://www.intogen.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6943a2",
   "metadata": {},
   "source": [
    "## Load Known Cancer Driver Genes\n",
    "\n",
    "For evaluation, we need reference sets of known cancer drivers from:\n",
    "- **COSMIC Cancer Gene Census**\n",
    "- **IntOGen**\n",
    "- **Bailey et al. 2018** (comprehensive driver study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eaf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "results_dir = Path(\"../data/evaluation\")\n",
    "reference_dir = Path(\"../data/evaluation/reference\")\n",
    "\n",
    "# Create directories\n",
    "reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Reference data directory: {reference_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39506cfb",
   "metadata": {},
   "source": [
    "## Setup Paths and Load Data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
